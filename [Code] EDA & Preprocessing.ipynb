{"cells":[{"cell_type":"markdown","metadata":{"id":"ZcMk2xAIAPqF"},"source":["# **Libraries**\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"777LThOb_yQf"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","import numpy as np\n","import pandas as pd\n","from pandas import datetime\n","import matplotlib.pyplot as plt\n","import seaborn as sns # advanced vizs\n","%matplotlib inline\n","from statsmodels.distributions.empirical_distribution import ECDF\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","import plotly.express as px\n","import plotly.io as pio\n","import plotly.express as px\n","import plotly.graph_objects as go\n","import plotly.figure_factory as ff\n","from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n","from plotly.subplots import make_subplots"]},{"cell_type":"markdown","metadata":{"id":"uWPGN8UMBsEk"},"source":["# Background & Problem Statement"]},{"cell_type":"markdown","metadata":{"id":"yA85yg3aBwyG"},"source":["ROSSMANN is a germany large drug store chain that operates accross the Europe, the objective of this study is to predict 6 weeks of daily sales for 1,115 ROSSMANN drug stores located across Germany, as reliable sales forecasts enable store managers to increase the overall productivity and profitability of the retail business and improve their customer satisfaction.  \n","\n","However, the challenges in this sales forecasting problem are to take into account various types of factors and to deal with missing data from historical records.\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"46DV6qO4FVi4"},"source":["# 'Stores' Dataset's Columns and Description"]},{"cell_type":"markdown","metadata":{"id":"J14l4RmFFYmE"},"source":["1.   **Store**:  the anonymised store number\n","2.   **StoreType**: 4 different store models: a, b, c, d\n","3.   **Assortment**: an assortment level: a = basic, b = extra, c = extended\n","4.   **CompetitionDistance**:distance in meters to the nearest competitor store\n","5.   **CompetitionOpenSinceMonth**: the approximate month of the time when the nearest competitor was opened\n","6.   **CompetitionOpenSinceYear**: the approximate year of the time when the nearest competitor was opened   \n","7.   **Promo2**: a continuing and consecutive promotion, e.g., a coupon based mailing campaign, for some stores: 0 = store is not participating, 1 = store is participating\n","8.   **Promo2SinceWeek**: the calendar week when the store started participating in Promo2\n","9.   **Promo2SinceYear**: the year when the store started participating in Promo2\n","10.   **PromoInterval** : the consecutive intervals in which Promo2 is restarted, naming the months the promotion is started anew.e.g.,\"Feb,May,Aug,Nov\" means each round of the coupon based mailing campaign starts in February, May, August, November of any given year for that store, as the coupons, mostly for a discount on certain products are usually valid for three months, and a new round of mail needs to be sent to customers just before those coupons have expired\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"b9NJOaoOG7aX"},"source":["# 'Train' and 'Test' Datasets' Columns and Description"]},{"cell_type":"markdown","metadata":{"id":"gxmDHzEJG_oZ"},"source":["  \n","\n","1.   **Store**: the anonymised store number\n","2.   **DayOfWeek**: the day of the week: 1 = Monday, 2 = Tuesday, …\n","3.   **Date**: the given date\n","4.  **Sales** : the turnover on a given day*\n","5.  **Customers** : the number of customers on a given day*\n","6.   **Open**: an indicator for whether the store was open on that day: 0 = closed, 1 = open\n","7.   **Promo**: indicates whether a store is running a store-specific\n","promo on that day\n","8.   **StateHoliday**: indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends.\n","      a = public holiday,\n","      b = Easter holiday,\n","      c = Christmas,\n","      0 = none\n","9.   **SchoolHoliday**: indicates if the (Store, Date) was affected by the closure of public schools\n","\n","*test.csv is similar with train.csv except that Sales and Customers are unknown for the period of 01/08/2015 to 17/09/2015. *italicized text*\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-viGmCFrCC97"},"source":["# Objective"]},{"cell_type":"markdown","metadata":{"id":"fYwqI4vaCGC-"},"source":["Forecast sales for the period of August 1st to September 17th, 2015, using historical data from January 1st, 2013, to July 31st, 2015 by employing the most appropriate EDA technique, data preprocessing, suitable ML model, and evaluate performance metrics for accurate sales predictions."]},{"cell_type":"markdown","source":["# Datasets Loading"],"metadata":{"id":"D5zBhksVp77S"}},{"cell_type":"markdown","metadata":{"id":"Z5CSDofXQI8e"},"source":["## Data Loading 'Store.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dyrRIc9PQQD4"},"outputs":[],"source":["store = pd.read_csv('store.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WlVJYGcnRgsX"},"outputs":[],"source":["print('Total row of store_df: ',len(store))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hg2UwOXmRqyF"},"outputs":[],"source":["print('Any NA at df_store: ')\n","print(store.isna().any())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wXM5OQv7eEQB","collapsed":true},"outputs":[],"source":["store.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"mciace6eQ7Rw"},"source":["## Data Loading 'train.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15QVtdfRQ-tl"},"outputs":[],"source":["train = pd.read_csv('train.csv')"]},{"cell_type":"code","source":["print('Total row of train: ',len(train))"],"metadata":{"id":"z7mukUL5zBwS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LuaLGAaDRNXI"},"outputs":[],"source":["train.head()"]},{"cell_type":"code","source":["# Assuming 'Date' column is not in datetime format, convert it first\n","train['Date'] = pd.to_datetime(train['Date'], format='%d/%m/%Y')\n","\n","subset = train[(train['Date'] >= '2015-01-08') & (train['Date'] <= '2015-01-12')]\n","\n","# Display or use the filtered DataFrame as needed\n","subset"],"metadata":{"id":"ipuOwlyMxFl9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iyf4xaGeTCUh"},"outputs":[],"source":["unique_store_count = train['Store'].nunique()\n","\n","print(\"Number of unique values in the 'Store' column:\", unique_store_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_UIkPmjRwI7"},"outputs":[],"source":["print('Any NA at train_df: ')\n","print(train.isna().any())"]},{"cell_type":"markdown","metadata":{"id":"ZxDNuakPSH2h"},"source":["## Data Loading 'test.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01p-OhwGSKxs"},"outputs":[],"source":["test_df = pd.read_csv('test.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7rx4qcSmSO7P"},"outputs":[],"source":["print('Total row of test_df: ',len(test_df))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpwMuJxsSR1B"},"outputs":[],"source":["print('Any NA at test_df: ')\n","print(test_df.isna().any())"]},{"cell_type":"code","source":["test_df"],"metadata":{"id":"N_BMPpaUkeY_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start_date = '13/08/2015'\n","end_date = '14/08/2015'\n","\n","# Assuming 'Date' is the name of your date column\n","selected_rows = test_df[(test_df['Date'] >= start_date) & (test_df['Date'] <= end_date)]\n","\n","selected_rows\n"],"metadata":{"id":"SHYMY1x2kT-b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5vTdxkr2D0Qp"},"source":["# 2. Exploratory Data Analysis"]},{"cell_type":"markdown","source":["## 2.1 Data Cleaning & Preparation"],"metadata":{"id":"T4WfVKx6qvKL"}},{"cell_type":"markdown","source":["### 2.1.1 Data Imputation (store.csv)"],"metadata":{"id":"bdG4bdS1fiss"}},{"cell_type":"markdown","metadata":{"id":"A7ev0F72mYMn"},"source":["#### Missing value on Promo2SinceWeek,Promo2SinceYear, PromoInterval columns needed a reasonable approach to impute it. However, there are no additional knowledge or context available to impute those values accurately. Hence, we prefer to fill it with 0 implying that they did not follow any consecutive promotion."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Potp3iqmkK2P"},"outputs":[],"source":["promotion_to_fill_with_0 = ['Promo2SinceWeek','Promo2SinceYear','PromoInterval']\n","\n","# Fill missing values in specified columns with 0\n","store[promotion_to_fill_with_0] = store[promotion_to_fill_with_0].fillna(0)\n"]},{"cell_type":"markdown","source":["#### Then, missing value on CompetitionDistance, CompetitionOpenSinceMonth, and CompetitionOpenSinceYear can be imputed as it can be interpreted based on the their on patterns as follow.\n","\n","####CompetitionDistance: impute by the median distance as the dataset is skewed (not normally distributed),CompetitionOpenSinceMonth: impute by the most frequent month competitor opened their store,CompetitionOpenSinceYear: impute by the most frequent year competitor opened their store"],"metadata":{"id":"p2jsXdFNeFhl"}},{"cell_type":"code","source":["competition_distance = store['CompetitionDistance']\n","\n","plt.hist(competition_distance, bins='auto', color='blue', alpha=0.7, rwidth=0.85)\n","plt.title('Competition Distance Histogram')\n","plt.xlabel('Competition Distance')\n","plt.ylabel('Frequency')\n","plt.grid(axis='y', alpha=0.75)\n","plt.show()"],"metadata":{"id":"TrhACKmyfP7y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Impute CompetitionDistance by the median distance\n","avg_distance = store['CompetitionDistance'].mean()\n","store['CompetitionDistance'].fillna(avg_distance, inplace=True)\n","\n","# Impute CompetitionOpenSinceMonth by the most frequent month\n","most_freq_month = store['CompetitionOpenSinceMonth'].mode()[0]\n","store['CompetitionOpenSinceMonth'].fillna(most_freq_month, inplace=True)\n","\n","# Impute CompetitionOpenSinceYear by the most frequent year\n","most_freq_year = store['CompetitionOpenSinceYear'].mode()[0]\n","store['CompetitionOpenSinceYear'].fillna(most_freq_year, inplace=True)"],"metadata":{"id":"5pfD8JypfVo2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Any NA at store_df: ',store.isna().any())"],"metadata":{"id":"1_34P0P-fZQ0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZQJaMi7xSZDQ"},"source":["### 2.1.2 Data Merge 'store.csv' and 'train.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zyqqnaQ3SfBh"},"outputs":[],"source":["# Merging 'store_df' with 'train_df' based on the 'Store' column as merged_df\n","merged_df = pd.merge(train, store, on='Store', how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d8j3Ics6TwM5"},"outputs":[],"source":["merged_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a4vdvc05T5WP"},"outputs":[],"source":["print('Total row of merged_df: ',len(merged_df))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hy8NOhzPUvdV"},"outputs":[],"source":["# Change 'date' column type to datetime\n","merged_df['Date'] = pd.to_datetime(merged_df['Date'], format='%d/%m/%Y')\n","merged_df.info()"]},{"cell_type":"code","source":["latest_date_merged = merged_df['Date'].max()\n","print(\"Latest Date in 'Date' Column:\", latest_date_merged)\n"],"metadata":{"id":"qczksqSQ0dCF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UhcSP5vPlpNK"},"outputs":[],"source":["# Get the count of unique values in each column\n","unique_value_counts = merged_df.nunique()\n","\n","# Display the count of unique values in each column\n","print(\"Count of unique values in each column:\")\n","print(unique_value_counts)"]},{"cell_type":"code","source":["# Assuming 'Date' is a column in your DataFrame\n","date_cutoff = '2015-07-31'\n","\n","# Convert 'Date' column to datetime if necessary\n","merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n","\n","# Filter rows in the DataFrame based on the 'Date' column\n","merged_df = merged_df[merged_df['Date'] <= date_cutoff]\n","\n","\n"],"metadata":{"id":"dmxGPT2yOzKR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["merged_result_agg = merged_df.set_index('Date').resample('D')['Sales'].mean().reset_index()\n","fig_merged = px.line(merged_result_agg, x='Date', y='Sales', title='Actual Sales on Merged dataset')\n","fig_merged.show()"],"metadata":{"id":"jLnTo4zH_tpE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.express as px\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","from plotly.subplots import make_subplots\n","import plotly.graph_objects as go\n","\n","# Assuming sales_a, sales_b, sales_c, sales_d are your time series data for different categories\n","\n","# Perform seasonal decomposition\n","decomposition_a = seasonal_decompose(sales_a, model='additive', period=365)\n","decomposition_b = seasonal_decompose(sales_b, model='additive', period=365)\n","decomposition_c = seasonal_decompose(sales_c, model='additive', period=365)\n","decomposition_d = seasonal_decompose(sales_d, model='additive', period=150)\n","\n","# Create subplots in a 2x2 grid\n","fig = make_subplots(rows=2, cols=2, subplot_titles=['Sales Store Type A', 'Sales Store Type B', 'Sales Store Type C', 'Sales Store Type D'],\n","                    shared_xaxes=True, vertical_spacing=0.1)\n","\n","# Add traces for trend to each subplot\n","fig.add_trace(go.Scatter(x=decomposition_a.trend.index, y=decomposition_a.trend, mode='lines', name='Trend A'), row=1, col=1)\n","fig.add_trace(go.Scatter(x=decomposition_b.trend.index, y=decomposition_b.trend, mode='lines', name='Trend B'), row=1, col=2)\n","fig.add_trace(go.Scatter(x=decomposition_c.trend.index, y=decomposition_c.trend, mode='lines', name='Trend C'), row=2, col=1)\n","fig.add_trace(go.Scatter(x=decomposition_d.trend.index, y=decomposition_d.trend, mode='lines', name='Trend D'), row=2, col=2)\n","\n","# Update layout\n","fig.update_layout(height=600, width=1400, title_text='Trend of Sales in Different Categories', showlegend=True)\n","\n","# Add common x-axis title using annotation\n","fig.add_annotation(text=\"Date\", xref=\"paper\", yref=\"paper\", x=0.5, y=-0.1, showarrow=False)\n","\n","# Show the figure\n","fig.show()\n"],"metadata":{"id":"Att6ZtUM6fP_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2.1.3 Checking for Missing Values of the the final train set (merged)"],"metadata":{"id":"Nq7wnbdeo4EI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0v7AEPw1eJgw"},"outputs":[],"source":["print('Any NA at merged_df: ',merged_df.isna().any())"]},{"cell_type":"markdown","metadata":{"id":"a-fcwTYXEcw9"},"source":["### 2.1.4 Summary and detail of the Train set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LFoSx0JPeVLv"},"outputs":[],"source":["print('Number of training rows:', merged_df.index.nunique())\n","print('Number of test rows:', test_df.index.nunique())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V0hk5dQcNUhh"},"outputs":[],"source":["merged_df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y3jWVYsZNkPO"},"outputs":[],"source":["merged_df.head(6)"]},{"cell_type":"code","source":["latest_date_merged = merged_df['Date'].max()\n","print(\"Latest Date in 'Date' Column:\", latest_date_merged)\n"],"metadata":{"id":"BR6FefI7wbx5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8snI0dsoQjd"},"outputs":[],"source":["merged_df.info()"]},{"cell_type":"markdown","source":["## 2.2 Exploratory Data Analysis (EDA)"],"metadata":{"id":"MaC5Pj7brgRJ"}},{"cell_type":"markdown","metadata":{"id":"JpNw-bwlE-j_"},"source":["### 2.2.1 Target Variable: Sales"]},{"cell_type":"markdown","metadata":{"id":"uL01xFJcF76k"},"source":["#### Normality Checking"]},{"cell_type":"markdown","source":["First, I check the normality for the relationship between Assorment VS Sales and Store VS Sales for several purpose: distribution understanding, statistical assumption, identifying outliers, and decision support."],"metadata":{"id":"IzQlhdGKq3FL"}},{"cell_type":"code","source":["# Assuming 'Assortment' and 'Sales' are columns in merged_df\n","unique_Assortment = merged_df['Assortment'].unique()\n","\n","# Filter the data for the selected Assortment\n","subset_df = merged_df[merged_df['Assortment'].isin(unique_Assortment)]\n","\n","# Checking normality using box plots\n","plt.figure(figsize=(5, 5))\n","plt.ylim(0, 25000)\n","dep_boxplot = sns.boxplot(x=\"Assortment\", y=\"Sales\", data=subset_df)\n","plt.show()\n"],"metadata":{"id":"eKbr4ddDnHJW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The distributions of sales for assortments ‘a’ and ‘c’ appear to be skewed towards lower values, and therefore, they do not seem to be normally distributed. The distribution of sales for assortment ‘b’ appears more balanced, but the presence of an outlier could affect its normality."],"metadata":{"id":"FiTWNNLmpvlA"}},{"cell_type":"code","source":["# Assuming 'Store' and 'Sales' are columns in merged_df\n","unique_stores = merged_df['Store'].unique()\n","\n","# Select a random subset of 20 stores\n","random_stores = random.sample(list(unique_stores), 100)\n","\n","# Filter the data for the selected stores\n","subset_df = merged_df[merged_df['Store'].isin(random_stores)]\n","\n","# Checking normality using box plots\n","plt.figure(figsize=(40, 10))\n","plt.ylim(0, 7000)\n","dep_boxplot = sns.boxplot(x=\"Store\", y=\"Sales\", data=subset_df)\n","plt.show()\n"],"metadata":{"id":"vEBaU34HhfYj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X3Swu59tpr0k"},"source":["Each store's sales differs in terms of range & outlier presence. it appears that the distribution of sales across different stores does not seem to be normal. There are noticeable variations in sales among different stores; some have very high sales while others have low. This suggests that there could be potential outliers where certain stores have significantly higher or lower sales than others."]},{"cell_type":"markdown","metadata":{"id":"9-Z0ptHUFM7p"},"source":["#### Analysis on Total Sales"]},{"cell_type":"markdown","source":["#### Monthly Average Sales"],"metadata":{"id":"B6WbhS5h2Q9n"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"omzIJcGSU3hz"},"outputs":[],"source":["# Monthly Average Sales\n","\n","# Set month\n","train_time = merged_df.set_index('Date').resample('M').Sales.mean()\n","train_time = pd.DataFrame(train_time)\n","train_time['year'] = train_time.index.year\n","\n","\n","fig = px.line(train_time, x=train_time.index, y='Sales', color='year', title='Monthly Average Sales')\n","fig.update_layout(width=900, height=500)"]},{"cell_type":"code","source":["# Monthly Total Sales\n","\n","# Set month\n","train_time = merged_df.set_index('Date').resample('M').Sales.sum()\n","train_time = pd.DataFrame(train_time)\n","train_time['year'] = train_time.index.year\n","\n","fig = px.line(train_time, x=train_time.index, y='Sales', color='year', title='Monthly Total Sales')\n","fig.update_layout(width=900, height=500)\n","\n","# If you want to show the x-axis as months instead of the full date\n","fig.update_layout(xaxis=dict(type='category'))\n"],"metadata":{"id":"nITGmGYI8fv8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The monthly average of daily sales of all stores increased every year, with the trends always rising and hit the highest peak every December. The sales generally started plumetting in August and back rising in early September each year—a fact that may need to be considered later during the modelling process when choosing time lags.\n","\n"],"metadata":{"id":"Wn7c4geWr-_6"}},{"cell_type":"markdown","source":["#### Annual Average Sales"],"metadata":{"id":"FXmSlrqu2ZGr"}},{"cell_type":"code","source":["# Annual Average Sales\n","train_time_year = merged_df.set_index('Date').resample('Y').Sales.mean()\n","train_time_year = pd.DataFrame(train_time_year)\n","train_time_year['year'] = train_time_year.index.year\n","fig = px.line(train_time_year, x=train_time_year.index, y='Sales', title = 'Annual Average Sales')\n","fig.update_layout(width=900, height=500)"],"metadata":{"id":"3ko3i9zm1--B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Day of Week Average Sales"],"metadata":{"id":"3JWMqM8s2ctw"}},{"cell_type":"code","source":["merged_day = merged_df.copy()\n","merged_day['year'] = merged_df.Date.dt.year #add year column\n","merged_day['DayOfWeek'] = merged_df.Date.dt.dayofweek+1 #add day of week column\n","day_gb = merged_day.groupby(['year', 'DayOfWeek'])['Sales'].mean().reset_index()\n","px.line(day_gb, x='DayOfWeek', y='Sales', color='year', title = 'Day of Week Average Sales')"],"metadata":{"id":"cX-YaQ3Q2cMx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.express as px\n","\n","# Copy the DataFrame\n","merged_day = merged_df.copy()\n","\n","# Add year column\n","merged_day['year'] = merged_df.Date.dt.year\n","\n","# Add day of week column\n","merged_day['DayOfWeek'] = merged_df.Date.dt.dayofweek + 1\n","\n","# Group by year, day of week, and Promo\n","day_gb = merged_day.groupby(['year', 'DayOfWeek', 'Promo'])['Sales'].mean().reset_index()\n","\n","# Create separate figures based on the 'Promo' column\n","fig_promo_0 = px.line(day_gb[day_gb['Promo'] == 0], x='DayOfWeek', y='Sales', color='year',\n","                      title='Day of Week Average Sales (Promo 0)')\n","\n","fig_promo_1 = px.line(day_gb[day_gb['Promo'] == 1], x='DayOfWeek', y='Sales', color='year',\n","                      title='Day of Week Average Sales (Promo 1)')\n","\n","# Show the figures\n","fig_promo_0.show()\n","fig_promo_1.show()\n"],"metadata":{"id":"eyRHTANVFf40"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Looking at average sales on specific days of the week, their lowest points happened in weekend (day 7), before ascending back on every first day of the week. However, we may also observed that the annual sales and day-of-week have distinct sales pattern."],"metadata":{"id":"TCJf6KwV278z"}},{"cell_type":"markdown","source":["#### Open Days"],"metadata":{"id":"JHS5sIeuRHks"}},{"cell_type":"code","source":["# Open Days: Is there any day where the stores close?\n","merged_df_time = merged_df[['Date', 'Sales']].groupby('Date').mean()\n","merged_df_time['open'] = 1\n","full_range = pd.date_range(start=merged_df_time.index.min(), end=merged_df_time.index.max(), freq='D')\n","full_df = pd.DataFrame(index=full_range)\n","\n","full_df = full_df.merge(merged_df_time['open'], left_index=True, right_index=True, how='left').fillna(0)\n","\n","fig = px.line(full_df, x=full_df.index, y='open', title='Open Days')\n","fig.update_layout(width=700, height=700)"],"metadata":{"id":"uddNEqx0REk8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From Figure above, the sales accross all stores are shown to be always ongoing throughout the year. Hence, suggesting despite the SchoolHoliday / StateHoliday variable, not all the stores were closed."],"metadata":{"id":"hCghQDbhS9CX"}},{"cell_type":"code","source":["print('The store holidays are on', pd.date_range(start=merged_df_time.index.min(), end=merged_df_time.index.max(), freq='D').difference(merged_df_time.index).values)"],"metadata":{"id":"9n0enTmaSJsR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stores_with_zero_sales = merged_df[merged_df['Sales'] == 0]\n","print(\"Rows with Sales equal to 0:\")\n","stores_with_zero_sales"],"metadata":{"id":"59hq-jJkUZrr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate average total number of days with 'Sales' equal to 0 for each store\n","average_days_with_zero_sales = stores_with_zero_sales.groupby('Store')['Date'].count().mean() / 2.5\n","\n","print(\"Average total days per-year with Sales equal to 0 for each Store:\", average_days_with_zero_sales)"],"metadata":{"id":"yAzKzCBZV-5M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["However, it doesn't mean that *each* store always has sales, in a year the store roughly were having 62 days that having 0 sales. These date and store who were having 0 sales doesn't have any pattern, mean it can be considered as an outliers"],"metadata":{"id":"QT4XuC9_Vm1Z"}},{"cell_type":"markdown","source":["#### Stores with 0 Sales"],"metadata":{"id":"B3de9H-dgsqz"}},{"cell_type":"code","source":["# Count occurrences of each store\n","store_counts = stores_with_zero_sales['Store'].value_counts()\n","\n","# Identify potential outliers in occurrences using IQR\n","Q1 = store_counts.quantile(0.25)\n","Q3 = store_counts.quantile(0.75)\n","IQR = Q3 - Q1\n","\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","potential_outliers = store_counts[(store_counts < lower_bound) | (store_counts > upper_bound)]\n","\n","# Plot box plot to visualize occurrences distribution and potential outliers\n","plt.figure(figsize=(3, 5))\n","sns.boxplot(store_counts)\n","plt.title('Box Plot of Count of Store with 0 Sales Occurrences')\n","plt.xlabel('Occurrences')\n","plt.ylabel('Count of Days with 0 Sales')\n","plt.show()"],"metadata":{"id":"01aCRfxSfnBX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select stores with more than 180 occurrences of Sales equal to 0\n","selected_stores = store_counts[store_counts > 180]\n","\n","# Print the result\n","print(\"Stores with more than 180 occurrences of Sales equal to 0:\")\n","print(selected_stores)\n","\n","# Plot occurrences using a bar plot\n","selected_stores.plot(kind='bar', rot=0, color='skyblue')\n","plt.title('Stores with more than 180 occurrences of Sales equal to 0 throughout 2013-Jun2015')\n","plt.xlabel('Store')\n","plt.ylabel('Count of Days 0 Sales')\n","plt.show()"],"metadata":{"id":"uu8riDMeXq1G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["18 Stores observed having occurences 0 sales more than 180 days throughout the dataset (January 2013 - 31 July 2015), we will later consider these Store to be excluded for the next phase, as it shows an anomaly behavior and affecting data quality for model generalization later."],"metadata":{"id":"feIV1AM7ZzEe"}},{"cell_type":"code","source":["# Define a custom color palette\n","custom_palette = {'a': 'blue', 'b': 'green', 'c': 'orange', 'd': 'red'}\n","\n","# Set the order of the columns based on StoreType\n","store_type_order = ['a', 'b', 'c', 'd']\n","\n","# Plot\n","sns.catplot(data=train_store, x='Month', y=\"Sales\",\n","            col='StoreType',  # per store type in cols\n","            hue='StoreType',\n","            row='Promo',  # per promo in the store in rows\n","            kind='point',\n","            palette=custom_palette,\n","            col_order=store_type_order)\n","\n","# Customize the layout\n","plt.subplots_adjust(top=0.9)\n","plt.suptitle('Sales Based on StoreType and Promo')\n","plt.show()\n"],"metadata":{"id":"ylr-xKwrP3Yg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute the correlation matrix\n","# exclude 'Open' variable\n","corr_all = train_store.drop('Open', axis = 1).corr()\n","\n","# Generate a mask for the upper triangle\n","mask = np.zeros_like(corr_all, dtype = np.bool)\n","mask[np.triu_indices_from(mask)] = True\n","\n","# Set up the matplotlib figure\n","f, ax = plt.subplots(figsize = (11, 9))\n","\n","# Draw the heatmap with the mask and correct aspect ratio\n","sns.heatmap(corr_all, mask = mask,\n","            square = True, linewidths = .5, ax = ax, cmap = \"coolwarm\")\n","plt.show()"],"metadata":{"id":"twVxMYR91n6J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sale per customer trends\n","sns.catplot(data = train_store, x = 'DayOfWeek', y = \"Sales\",\n","               col = 'Promo',\n","               row = 'Promo2',\n","               hue = 'Promo2',\n","               palette = 'RdPu',\n","            kind = 'point')"],"metadata":{"id":"cjCu9cn_1sjY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.2.2 Univariate & Multivariate Analysis"],"metadata":{"id":"WEW-HVnBhLKD"}},{"cell_type":"markdown","source":["### Store Performance Analysis"],"metadata":{"id":"DnZ20fYF3yBg"}},{"cell_type":"code","source":["#store's daily sales performance\n","merged_df['Store'] = merged_df['Store'].astype('category')\n","train_store = merged_df.groupby(['Date', 'Store']).Sales.mean().reset_index()\n","\n","\n","fig = px.line(train_store, x='Date', y='Sales', color='Store', title='Daily Sales Average by Store Number')\n","fig.update_layout(width=1500, height=700)"],"metadata":{"id":"mVFzAgmL32oO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#boxplot\n","px.box(train_store, x='Store', y='Sales', color='Store', title='Average Sales by Store Number')"],"metadata":{"id":"V_z6NnP-4zNX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 'Customer' Analysis"],"metadata":{"id":"tnsRB2EIO6Jq"}},{"cell_type":"markdown","source":["'Customer' must be one variable that having positive correlation with Sales."],"metadata":{"id":"b225e0b1PApd"}},{"cell_type":"code","source":["plt.figure(figsize=(10, 6))\n","sns.regplot(x='Customers', y='Sales', data=merged_df, scatter_kws={'alpha':0.3}, line_kws={'color': 'red'})\n","plt.title('Scatter Plot with Regression Line: Sales vs Number of Customers')\n","plt.xlabel('Number of Customers')\n","plt.ylabel('Sales')\n","plt.show()"],"metadata":{"id":"HdQd7-CyPH2Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 'Promo' Analysis"],"metadata":{"id":"QNnw8_SHw-rY"}},{"cell_type":"markdown","source":["'Promo' become one of the variable is commonly suspected having high of influence towards the Sales. Hence, analyze this variable first will be reasonable"],"metadata":{"id":"eszosThnxXul"}},{"cell_type":"code","source":["import plotly.express as px\n","\n","# Assuming 'Date' is already in datetime format\n","special_offer = merged_df.set_index('Date').resample('M')['Promo', 'Sales'].mean().reset_index()\n","\n","# Create a figure with bar plot for 'Promo' and line plot for 'Sales'\n","fig = px.bar(special_offer, x='Date', y='Promo', title='Promo and Sales Trend')\n","fig.add_trace(px.line(special_offer, x='Date', y='Sales').update_traces(yaxis='y2', line=dict(color='blue')).data[0])\n","\n","# Set y-axis titles\n","fig.update_layout(yaxis_title='Promo', width=1000, height=500)\n","\n","# Add a second y-axis for 'Sales'\n","fig.update_layout(\n","    yaxis2=dict(\n","        overlaying='y',\n","        side='right',\n","        title='Sales'\n","    )\n",")\n","\n","# Show the plot\n","fig.show()\n"],"metadata":{"id":"OLq7P2vozxhE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From figure above, we might observed that the Promo has correllation with the Sales that we will include in our feature engineering and modeling stage later , while also need to be noted that some of the trend that not relevant that may be caused Promo Interval variable."],"metadata":{"id":"K3L7Q5Cpxv5a"}},{"cell_type":"markdown","source":["### 'Competition Distance' Analysis"],"metadata":{"id":"9M-wLtFxxK3W"}},{"cell_type":"markdown","source":["Next, Competition Distance become the next variable will be discussed as hypotetically having impact to the Sales."],"metadata":{"id":"RhrQGnPh0ygE"}},{"cell_type":"code","source":["plt.figure(figsize=(10, 6))\n","sns.regplot(x='CompetitionDistance', y='Sales', data=merged_df, scatter_kws={'alpha':0.3}, line_kws={'color': 'red'})\n","plt.title('Scatter Plot with Regression Line: Sales vs. Competition Distance')\n","plt.xlabel('Competition Distance')\n","plt.ylabel('Sales')\n","plt.show()"],"metadata":{"id":"30gm4fxDuV1g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From figure above, turns out Competition Distance is relatively having negative correllation with the Sales as most of the highest sales is produced when having a near competitor."],"metadata":{"id":"Su84lNvn1Qyj"}},{"cell_type":"markdown","source":["### 'Assortment' Analysis"],"metadata":{"id":"Q7vErXje7am3"}},{"cell_type":"code","source":["#product's daily sales performance\n","merged_prod = merged_df.groupby(['Date', 'Assortment']).Sales.mean().reset_index()\n","fig = px.line(merged_prod, x='Date', y='Sales', color='Assortment', title='Sales by Assortment Type')\n","fig.update_layout(width=1000, height=400)"],"metadata":{"id":"A5WvOxLtn7cN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#boxplot\n","merged_prod = merged_df.groupby(['Date', 'Assortment']).Sales.mean().reset_index()\n","fig = px.box(merged_prod, x='Assortment', y='Sales', color='Assortment', title='Average Sales by Assortment Type')\n","fig.update_layout(width=700, height=500)"],"metadata":{"id":"NSEn4RwwqsTn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 'Store Type' Analysis"],"metadata":{"id":"E7jS_krNvDTS"}},{"cell_type":"code","source":["#product's daily sales performance\n","merged_prod = merged_df.groupby(['Date', 'StoreType']).Sales.mean().reset_index()\n","fig = px.line(merged_prod, x='Date', y='Sales', color='StoreType', title='Sales by Store Type')\n","fig.update_layout(width=1000, height=400)"],"metadata":{"id":"GmnpmTR49fnA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#boxplot\n","merged_prod = merged_df.groupby(['Date', 'StoreType']).Sales.mean().reset_index()\n","fig = px.box(merged_prod, x='StoreType', y='Sales', color='StoreType', title='Average Sales by Store Type')\n","fig.update_layout(width=700, height=600)"],"metadata":{"id":"exhaFzZU9lz-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# opened stores with zero sales\n","zero_sales = train[(train.Open != 0) & (train.Sales == 0)]\n","print(\"In total: \", zero_sales.shape)\n","zero_sales.head(5)"],"metadata":{"id":"rsQ7muHiYwFp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Conclusion from EDA"],"metadata":{"id":"dysKfBDhw-Nq"}},{"cell_type":"markdown","source":["1. Annual sales and day-of-week sales have distinct sales patterns.\n","2. Despite having state and school Holiday, generally the ROSSMAN through its store accross the country is always having a sales activity everyday.\n","3. Promo has positive correlation to sales.\n","4. Competition Distance has negative correlation to sales.\n","5. Some stores, have only being consistent generating sales started around 8 October 2014. This strengthens the argument of\n","using only 8 October 2014 onward data for the modeling stage.\n","6. Assortment type C relatively do not have any pattern as the sales very fluctuative throughout the years."],"metadata":{"id":"d8u4YkB2xAyT"}},{"cell_type":"markdown","source":["# 3. Data Pre-Processing"],"metadata":{"id":"gJ_0PrU7r4vL"}},{"cell_type":"markdown","source":["## 3.1 Delete unnecessary records"],"metadata":{"id":"24TCvxJisreA"}},{"cell_type":"markdown","source":["The EDA revealed the issue of some variables that having unneccessary records and need to be deleted"],"metadata":{"id":"_5-SIfHAsA4i"}},{"cell_type":"markdown","source":["- Closed stores which has 0 sales occurences"],"metadata":{"id":"SP6xHdz0tx2p"}},{"cell_type":"code","source":["# Remove rows with 'Store' values in selected_stores.index\n","filtered_df = merged_df[(merged_df[\"Open\"] != 0) & (merged_df['Sales'] != 0)]\n"],"metadata":{"id":"Z9ELis4c3K8o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Overview = merged_df.set_index('Date').resample('D')['Sales'].mean().reset_index()\n","aa_merged = px.line(Overview, x='Date', y='Sales', title='Actual Sales on Merged dataset')\n","aa_merged.show()"],"metadata":{"id":"U2BeccUdCkEC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.2 Feature Engineering (Time-series feature)"],"metadata":{"id":"Y_wj7s8fyvWv"}},{"cell_type":"markdown","source":["### 3.2.2 Adding Sales Lag"],"metadata":{"id":"2w5ZD1075AFW"}},{"cell_type":"markdown","source":["Adding sales lag"],"metadata":{"id":"STJB0bVH9ND2"}},{"cell_type":"code","source":["prod_info_sorted = filtered_df.sort_values(by=['Store', 'StoreType','Assortment','Date'])\n","\n","prod_info_sorted['sales_lag7'] = prod_info_sorted.groupby(['Store', 'StoreType','Assortment'])['Sales'].shift(7) #lag7\n","prod_info_sorted['sales_lag12'] = prod_info_sorted.groupby(['Store', 'StoreType','Assortment'])['Sales'].shift(12) #lag14\n","prod_info_sorted['sales_lag14'] = prod_info_sorted.groupby(['Store', 'StoreType','Assortment'])['Sales'].shift(14) #lag21\n","prod_info_sorted['sales_lag24'] = prod_info_sorted.groupby(['Store', 'StoreType','Assortment'])['Sales'].shift(24) #lag90\n"],"metadata":{"id":"bqPkPrNCEBfa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prod_info_sorted"],"metadata":{"id":"eZ4IcwaG9DbT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.3 Encode"],"metadata":{"id":"pbXiOaiDy9yY"}},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","\n","prod_info_encoded = prod_info_sorted.copy()\n","label_encoder = LabelEncoder()\n","\n","# Encode 'Assortment'\n","assortment_values = prod_info_encoded['Assortment'].astype(str)\n","label_encoder.fit(assortment_values)\n","prod_info_encoded['encoded_Assortment'] = label_encoder.transform(assortment_values)\n","\n","# Encode 'StoreType'\n","store_type_values = prod_info_encoded['StoreType'].astype(str)\n","label_encoder.fit(store_type_values)\n","prod_info_encoded['encoded_StoreType'] = label_encoder.transform(store_type_values)\n","\n","# Encode 'PromoInterval'\n","promo_interval_values = prod_info_encoded['PromoInterval'].astype(str)\n","label_encoder.fit(promo_interval_values)\n","prod_info_encoded['encoded_PromoInterval'] = label_encoder.transform(promo_interval_values)\n","\n","prod_info_encoded = prod_info_encoded.sort_values(by=['Date', 'Store', 'Assortment']).reset_index(drop=True)"],"metadata":{"id":"uoII08y06Rl4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prod_info_encoded.head(5)"],"metadata":{"id":"bkWO6JjMY4TG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start_date = '2015-07-30'\n","end_date = '2015-07-31'\n","\n","# Assuming 'Date' is the name of your date column\n","selected_rows = prod_info_encoded[(prod_info_encoded['Date'] >= start_date) & (prod_info_encoded['Date'] <= end_date)]\n","\n","selected_rows.head(5)"],"metadata":{"id":"iZsqqbIhjdls"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prod_info_encoded.isna().any()"],"metadata":{"id":"Vq-3GBJ39siV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.5 Split into train and test(validation) dataset"],"metadata":{"id":"VIcoKbVncEA1"}},{"cell_type":"code","source":["\n","# Calculate the index to split the DataFrame\n","split_index = int(len(prod_info_encoded) * 0.8)\n","\n","# Create training set (80%)\n","train_preprocessed = prod_info_encoded.iloc[:split_index]\n","\n","# Create testing set (20%)\n","# Note: Now, the testing set only includes rows with 'Date' until 2015-07-31\n","test_preprocessed = prod_info_encoded.iloc[split_index:]\n","\n","# Print the shapes of the sets to verify the split\n","print(\"Train shape:\", train_preprocessed.shape)\n","print(\"Test shape:\", test_preprocessed.shape)\n"],"metadata":{"id":"Ug2qqv5ecNJe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For train_preprocessed\n","train_result_agg = train_preprocessed.set_index('Date').resample('D')['Sales'].mean().reset_index()\n","fig_train = px.line(train_result_agg, x='Date', y='Sales', title='Actual Sales on Train dataset')\n","fig_train.show()\n","\n","# For test_preprocessed\n","test_result_agg = test_preprocessed.set_index('Date').resample('D')['Sales'].mean().reset_index()\n","fig_test = px.line(test_result_agg, x='Date', y='Sales', title='Actual Sales on Test dataset')\n","fig_test.show()"],"metadata":{"id":"QV0v0WXo94Xr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.6 Export merged_df into CSV"],"metadata":{"id":"rkr7SjrQ0Q0q"}},{"cell_type":"code","source":["from google.colab import files\n","train_preprocessed.to_csv('train_preprocessed', index=False)\n","files.download('train_preprocessed')\n","\n","test_preprocessed.to_csv('test_preprocessed', index=False)\n","files.download('test_preprocessed')"],"metadata":{"id":"8ChYRHco9e-Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Extend the test.csv"],"metadata":{"id":"pQ8PnKyxltWG"}},{"cell_type":"code","source":["test_df['Date'] = test_df['Date'].str.strip()\n","test_df['Date'] = pd.to_datetime(test_df['Date'], format='%d/%m/%Y', errors='coerce')\n"],"metadata":{"id":"Lhn1TIsFr5j2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check if the following date can be converted to datetime correctly after stripping the whitespace"],"metadata":{"id":"ztWJu6KIs_PB"}},{"cell_type":"code","source":["start_date = '2015-08-10'\n","end_date = '2015-08-12'\n","selected_rows = test_df[(test_df['Date'] >= start_date) & (test_df['Date'] <= end_date)]\n","print(selected_rows)\n"],"metadata":{"id":"TeLYQInWsScV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert 'Date' column to datetime format in merged_df\n","merged_df['Date'] = pd.to_datetime(merged_df['Date'], errors='coerce')\n","\n","# Convert 'Date' column to datetime format in test_df\n","test_df['Date'] = pd.to_datetime(test_df['Date'], errors='coerce')\n","\n","# Concatenate the two DataFrames\n","extended_df = pd.concat([merged_df, test_df], ignore_index=True)\n","\n","# Sort the DataFrame by 'Date' in descending order\n","extended_df = extended_df.sort_values(by='Date', ascending=False)\n","\n","# Display the sorted and extended DataFrame\n","extended_df"],"metadata":{"id":"fyImSs2yiNJD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start_date = '2015-09-01'\n","end_date = '2015-09-07'\n","\n","# Assuming 'Date' is the name of your date column\n","selected_rows = extended_df[(extended_df['Date'] >= start_date) & (extended_df['Date'] <= end_date)]\n","\n","selected_rows"],"metadata":{"id":"w0_X-JPMkxP_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["extended_sorted = extended_df.fillna(0).sort_values(by=['Store', 'DayOfWeek', 'Date'])\n","\n","extended_sorted['sales_lag7'] = extended_sorted.groupby(['Store', 'DayOfWeek'])['Sales'].shift(7) #lag7\n","extended_sorted['sales_lag14'] = extended_sorted.groupby(['Store', 'DayOfWeek'])['Sales'].shift(14) #lag14\n","extended_sorted['sales_lag12'] = extended_sorted.groupby(['Store', 'DayOfWeek'])['Sales'].shift(12) #lag12\n","extended_sorted['sales_lag24'] = extended_sorted.groupby(['Store', 'DayOfWeek'])['Sales'].shift(24) #lag24\n","\n"],"metadata":{"id":"3DSQIfmlRKp-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filtered_df = extended_sorted[(extended_sorted['Date'] >= '2015-08-01') & (extended_sorted['Date'] <= '2015-09-17')]\n","sorted_df = filtered_df.sort_values(by='Date', ascending=True)\n","sorted_df"],"metadata":{"id":"02eu8KVTXslu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["extended_sorted = extended_sorted[(extended_sorted['Date'] >= '2015-08-01') & (extended_sorted['Date'] <= '2015-09-17')]\n"],"metadata":{"id":"oLOK8jd7Z_2P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","extended_sorted.to_csv('forecasting_df', index=False)\n","files.download('forecasting_df')"],"metadata":{"id":"hmjlh_KLZl7y"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["ZcMk2xAIAPqF","46DV6qO4FVi4","b9NJOaoOG7aX","tAmxEj8rB4kj","-viGmCFrCC97","Z5CSDofXQI8e","mciace6eQ7Rw","ZxDNuakPSH2h","JpNw-bwlE-j_","FXmSlrqu2ZGr","3JWMqM8s2ctw","DnZ20fYF3yBg","tnsRB2EIO6Jq","QNnw8_SHw-rY","9M-wLtFxxK3W","Q7vErXje7am3","E7jS_krNvDTS","dysKfBDhw-Nq"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}